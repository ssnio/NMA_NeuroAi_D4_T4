{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "execution": {},
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/neuromatch/course-content-template/blob/main/tutorials/W1D2_Template/W1D2_Tutorial1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a> &nbsp; <a href=\"https://kaggle.com/kernels/welcome?src=https://raw.githubusercontent.com/neuromatch/course-content-template/main/tutorials/W1D2_Template/W1D2_Tutorial1.ipynb\" target=\"_parent\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open in Kaggle\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Tutorial 4.(last part): Inductive Biases in Self-Attention Mechanisms\n",
    "\n",
    "**Week [1], Day [4]: [Micro-Circuits]**\n",
    "\n",
    "**By Sciencematch Academy** # update to the correct academy\n",
    "\n",
    "__Content creators:__ Names & Surnames\n",
    "\n",
    "__Content reviewers:__ Names & Surnames\n",
    "\n",
    "__Production editors:__ Names & Surnames\n",
    "\n",
    "<br>\n",
    "\n",
    "Acknowledgments: [ACKNOWLEDGMENT_INFORMATION]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "___\n",
    "\n",
    "Use a line (---) separator from title block to objectives. \n",
    "\n",
    "# Tutorial Objectives\n",
    "\n",
    "*Estimated timing of tutorial: [insert estimated duration of whole tutorial in minutes]*\n",
    "\n",
    "Transformers have been surprisingly successful in almost every modality in ML, surpassing every other deep-learning model in performance and generalization especially in sequence generation and self-supervised representation learning.\n",
    "\n",
    "But unlike MultiLayer fully connected Perceptrons (MLPs), Recurrent Neural Networks (RNNs), and Convolution Neural Nets (CNNs), transformer architecture seems to have no explicit inductive bias. This goes against the common understanding in machine learning, that to achieve good generalization, one must exploit the geometry of data and problem and build the necessary symmetries into the architecture.\n",
    "\n",
    "This raises the question of whether we can find a class of functions (i.e. set of problems) that transformers excel at in representing. To this end, here we try to present the results from [Inductive Biases and Variable Creation in Self-Attention Mechanisms](https://arxiv.org/abs/2110.10090) which shows that a single self-attention head can successfully learn to represent a sparse-function with an extremely efficient sample-complexity. Although the original paper is rather focused on the rigorous proof of the claims, in this tutorial we will show-case their findings and general message. *Throughout this tutorial, we will refer to the aforementioned paper as \"the paper\".*\n",
    "\n",
    "**Tutorial Learning Structure**\n",
    "* We first introduce class of *s-sparce binary AND functions*\n",
    "* You then train an MLP model on an s-sparse AND dataset for classification task\n",
    "* We present a basic self-attention architecture for classification (similar to those used in BERT and ViT)\n",
    "* Next you should train the transformer model on the s-sparse AND dataset for classification task\n",
    "* Finally, we compare the results from both architecture.\n",
    "\n",
    "**Tutorial Learning Objectives**\n",
    "By the end you should:\n",
    "* feel familiar with the self-attention architecture\n",
    "* \n",
    "\n",
    "**References**:\n",
    "- The transformer code is from \"https://github.com/MathInf/toroidal\" by Thomas Viehmann\n",
    "- The content and results are inspired by \"Edelman et al. (2022), [Inductive Biases and Variable Creation in Self-Attention Mechanisms](https://proceedings.mlr.press/v162/edelman22a.html)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Tutorial Slides \"link_id\"s will be added in below by the curriculum or production team. You do not need to do anything but leave the block of code below here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Tutorial slides\n",
    "\n",
    "# @markdown These are the slides for the videos in all tutorials today\n",
    "\n",
    "\n",
    "## Uncomment the code below to test your function\n",
    "\n",
    "#from IPython.display import IFrame\n",
    "#link_id = \"<YOUR_LINK_ID_HERE>\"\n",
    "\n",
    "print(\"If you want to download the slides: 'Link to the slides'\")\n",
    "      # Example: https://osf.io/download/{link_id}/\n",
    "\n",
    "#IFrame(src=f\"https://mfr.ca-1.osf.io/render?url=https://osf.io/download/{link_id}/?direct%26mode=render\", width=854, height=480)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Setup\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Install and import feedback gadget\n",
    "\n",
    "# note this is not relevant for climatematch at the moment\n",
    "\n",
    "!pip3 install vibecheck datatops --quiet\n",
    "\n",
    "from vibecheck import DatatopsContentReviewContainer\n",
    "def content_review(notebook_section: str):\n",
    "    return DatatopsContentReviewContainer(\n",
    "        \"\",  # No text prompt - leave this as is\n",
    "        notebook_section,\n",
    "        {\n",
    "            \"url\": \"https://pmyvdlilci.execute-api.us-east-1.amazonaws.com/klab\",\n",
    "            \"name\": \"sciencematch_sm\", # change the name of the course : neuromatch_dl, climatematch_ct, etc\n",
    "            \"user_key\": \"y1x3mpx5\",\n",
    "        },\n",
    "    ).render()\n",
    "\n",
    "# Define the feedback prefix: Replace 'weeknumber' and 'daynumber' with appropriate values, underscore followed by T(stands for tutorial) and the number of the tutorial\n",
    "# e.g., W1D1_T1\n",
    "feedback_prefix = \"W*weeknumber*D*daynumber*_T*tutorialNumber*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import random as pyrandom  # to avoid confusion with np.random\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Figure settings\n",
    "\n",
    "# logging.getLogger('matplotlib.font_manager').disabled = True\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina' # perfrom high definition rendering for images and plots\n",
    "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/main/nma.mplstyle\") # update this to match your course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Plotting functions\n",
    "def plot_loss_accuracy(t_loss, t_acc, v_loss = None, v_acc = None):\n",
    "    plt.figure(figsize=(15, 4))\n",
    "    plt.suptitle(\"Training and Validation for the Transformer Model\")\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(t_loss, label=\"Training loss\", color=\"red\")\n",
    "    if v_loss is not None:\n",
    "        # plt.plot(v_loss, label=\"Valididation loss\", color=\"blue\")\n",
    "        plt.scatter(len(t_loss)-1, v_loss, label=\"Validation loss\", color=\"blue\", marker=\"*\")\n",
    "        # plt.text(len(t_loss)-1, v_loss, f\"{v_loss:.3f}\", va=\"bottom\", ha=\"right\")\n",
    "    plt.yscale(\"log\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.xticks([])\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(t_acc, label=\"Training accuracy\", color=\"red\", linestyle=\"dotted\")\n",
    "    if v_acc is not None:\n",
    "        # plt.plot(v_acc, label=\"Validation accuracy\", color=\"blue\", linestyle=\"--\")\n",
    "        plt.scatter(len(t_acc)-1, v_acc, label=\"Validation accuracy\", color=\"blue\", marker=\"*\")\n",
    "        # plt.text(len(t_acc)-1, v_acc, f\"{v_acc:.3f}\", va=\"bottom\", ha=\"right\")\n",
    "    plt.xticks([])\n",
    "    plt.ylim(0, 1)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_samples(X_plot, y_plot, correct_ids):\n",
    "    n_samples, seq_length = X_plot.shape\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(16, 2.5), sharey=True)\n",
    "    rects = []\n",
    "    for ri in correct_ids:\n",
    "        rects.append(plt.Rectangle((ri-0.5, -0.5), 1, n_samples, edgecolor=\"red\", alpha=1.0, fill=False, linewidth=2))\n",
    "    axs[0].imshow(X_plot, cmap=\"binary\")\n",
    "    for rect in rects:\n",
    "        axs[0].add_patch(rect)\n",
    "    # axs[0].axis(\"off\")\n",
    "    axs[0].set_yticks([])\n",
    "    axs[0].set_xticks([])\n",
    "    axs[0].set_ylabel(\"Context\")\n",
    "    axs[0].set_xlabel(\"Samples\")\n",
    "    axs[1].imshow(y_plot, cmap=\"binary\")\n",
    "    axs[1].add_patch(plt.Rectangle((-0.5, -0.5), 1, n_samples, edgecolor=\"black\", alpha=1.0, fill=False, linewidth=2))\n",
    "    axs[1].yaxis.set_label_position(\"right\")\n",
    "    axs[1].set_ylabel(\"Labels\")\n",
    "    axs[1].set_yticks([])\n",
    "    axs[1].set_xticks([])\n",
    "    plt.subplots_adjust(wspace=-1)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_attention_weights(att_weights, correct_ids, context_length):\n",
    "    aw_flatten = att_weights.view(-1, context_length+1)\n",
    "    n_weights = aw_flatten.size(0)\n",
    "    fig, ax = plt.subplots(figsize=(9, 5))\n",
    "    for i in range(context_length+1):\n",
    "        ax.scatter(torch.full((n_weights, ), i) , aw_flatten[:, i], alpha=0.1, c='blue')\n",
    "    rects = []\n",
    "    for ri in correct_ids:\n",
    "        rects.append(plt.Rectangle((ri-0.5, 1e-6), 1.0, 2.0, edgecolor=\"red\", alpha=1.0, fill=False, linewidth=2))\n",
    "    for rect in rects:\n",
    "        ax.add_patch(rect)\n",
    "    plt.yscale(\"log\")\n",
    "    plt.ylim(1e-6, 2)\n",
    "    plt.title(\"Attention weights for the whole batch\")\n",
    "    plt.xlabel(\"Boolean input index t\")\n",
    "    plt.ylabel(\"Attention weight\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "#@title Data retrieval\n",
    "class s_Sparse_Boolean:  # 1-Dimensional AND\n",
    "    def __init__(self, T: int, s: int):\n",
    "        self.T = T # context length\n",
    "        self.s = s # sparsity\n",
    "        self.p = 0.5**(1.0/3.0)  # probability chosen for balanced data\n",
    "        self.f_i = None\n",
    "\n",
    "    def pick_an_f(self):\n",
    "        self.f_i = sorted(pyrandom.sample(range(self.T), 3))\n",
    "        self.others = list(i for i in range(self.T) if i not in self.f_i)\n",
    "\n",
    "    def generate(self, m: int, verbose: bool = False):\n",
    "        if self.f_i is None:\n",
    "            self.pick_an_f()\n",
    "        max_try = 100\n",
    "        i_try = 0\n",
    "        while i_try < max_try:\n",
    "            i_try += 1\n",
    "            X, y = torch.zeros(m, self.T), torch.zeros(m, 1)\n",
    "            X[torch.rand(m, self.T) < self.p] = 1\n",
    "            y[X[:, self.f_i].sum(dim=1) == self.s] = 1\n",
    "            if y.sum()/m < 0.4 or y.sum()/m > 0.6:\n",
    "                verbose and print(f\"Large imbalance in the training set {y.sum()/m}, retrying...\")\n",
    "                continue\n",
    "            else:\n",
    "                verbose and print(f\"Data-label balance: {y.sum()/m}\")\n",
    "            bad_batch = False\n",
    "            for i in self.f_i:\n",
    "                for o in self.others:\n",
    "                    if (X[:, i] == X[:, o]).all():\n",
    "                        verbose and print(f\"Found at least another compatible hypothesis {i} and {o}\")\n",
    "                        bad_batch = True\n",
    "                        break\n",
    "            if bad_batch:\n",
    "                continue\n",
    "            else:\n",
    "                break\n",
    "        else:\n",
    "            print(\"Could not find a compatible hypothesis\")\n",
    "        return X.long(), y.float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Helper functions\n",
    "\n",
    "class BinaryMLP(torch.nn.Module):\n",
    "    def __init__(self, in_dims, h_dims, out_dims, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.in_dims = in_dims\n",
    "        self.h_dims = h_dims\n",
    "        self.out_dims = out_dims\n",
    "\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        self.layers.append(torch.nn.Linear(in_dims, h_dims[0]))\n",
    "        torch.nn.init.normal_(self.layers[-1].weight, std=0.02)\n",
    "        torch.nn.init.zeros_(self.layers[-1].bias)\n",
    "        self.layers.append(torch.nn.GELU())\n",
    "        self.layers.append(torch.nn.Dropout(dropout))\n",
    "        for i in range(len(h_dims) - 1):\n",
    "            self.layers.append(torch.nn.Linear(h_dims[i], h_dims[i+1]))\n",
    "            torch.nn.init.normal_(self.layers[-1].weight, std=0.02)\n",
    "            torch.nn.init.zeros_(self.layers[-1].bias)\n",
    "            self.layers.append(torch.nn.GELU())\n",
    "            self.layers.append(torch.nn.Dropout(dropout))\n",
    "        self.layers.append(torch.nn.Linear(h_dims[-1], out_dims))\n",
    "        self.layers.append(torch.nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def bin_acc(y_hat, y):\n",
    "    \"\"\"\n",
    "    Compute the binary accuracy\n",
    "    \"\"\"\n",
    "    y_ = y_hat.round()\n",
    "    TP_TN = (y_ == y).float().sum().item()\n",
    "    FP_FN = (y_ != y).float().sum().item()\n",
    "    assert TP_TN + FP_FN == y.numel(), f\"{TP_TN + FP_FN} != {y.numel()}\"\n",
    "    return TP_TN / y.numel()\n",
    "\n",
    "\n",
    "def get_n_parameters(model: torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Get the number of learnable parameters in a model\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    for par in model.parameters():\n",
    "        i += par.numel()\n",
    "    return i\n",
    "\n",
    "\n",
    "def save_model(model):\n",
    "    torch.save(model.state_dict(), 'model_states.pt')\n",
    "\n",
    "\n",
    "def load_model(model):\n",
    "    model_states = torch.load('model_states.pt')\n",
    "    model.load_state_dict(model_states)\n",
    "\n",
    "def evaluator(model, criterion, X_v, y_v, device=\"cpu\"):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    X_v, y_v = X_v.to(device), y_v.to(device)\n",
    "    with torch.no_grad():\n",
    "        y_hat = model(X_v)\n",
    "        loss = criterion(y_hat.squeeze(), y_v.squeeze())\n",
    "        acc = bin_acc(y_hat, y_v)\n",
    "    return loss.item(), acc\n",
    "\n",
    "\n",
    "def trainer(model, optimizer, criterion, n_epochs, X_t, y_t, device=\"cpu\", verbose=False):\n",
    "    train_loss, train_acc = [], []\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    X_t, y_t = X_t.to(device), y_t.to(device)\n",
    "    for i in range(n_epochs):\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        y_hat = model(X_t)\n",
    "        loss_t = criterion(y_hat.squeeze(), y_t.squeeze())\n",
    "        loss_t.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        if (i + 1) % 10 == 0 or i == 0:\n",
    "            train_loss.append(loss_t.item())\n",
    "            train_acc.append(bin_acc(y_hat, y_t))\n",
    "    model.eval()\n",
    "    return train_loss, train_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Set random seed for `Python` and `PyTorch`\n",
    "\n",
    "# @markdown Executing `set_seed(seed=seed)` you are setting the seed\n",
    "\n",
    "def set_seed(seed=None):\n",
    "    if seed is None:\n",
    "        seed = pyrandom.choice([i for i in range(1, 128)])\n",
    "    pyrandom.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    print(f'Random seed {seed} has been set.')\n",
    "\n",
    "set_seed(seed=2014)  # Bahdanau et al. (2014)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Set device (GPU, MPS or CPU). Execute `set_device()`\n",
    "\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda:0\")  # NVIDIA GPU\n",
    "        print(\"GPU is enabled in this notebook. \\n\"\n",
    "              \"If you want to disable it, in the menu under `Runtime` -> \\n\"\n",
    "              \"`Hardware accelerator.` and select `None` from the dropdown menu\")\n",
    "    elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "        device = torch.device(\"mps\")  # Apple Silicon (Metal)\n",
    "        print(\"MPS (Apple Silicon Metal) is enabled in this notebook.\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"GPU is not enabled in this notebook. \\n\"\n",
    "              \"If you want to enable it, in the menu under `Runtime` -> \\n\"\n",
    "              \"`Hardware accelerator.` and select `GPU` from the dropdown menu\")\n",
    "    return device\n",
    "\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 1.1: Formulation of problem\n",
    "\n",
    "Although the goal of AI researchers is to find a model architecture that could solve any problem regardless of its geometry and modality, the current state of machine learning is to rather build or tweak existing models to accomodate the necessary symmetries of the dataset. The authors of the paper identify that self-attention transformers have an inductive bias for sparse functions. They name this inductive bias as *sparse variable creation*.\n",
    "\n",
    "The sparse function we will use in this tutorial is the 3-sparse AND function. For a given sequence of length $T$ (context length) and three pre-selected unique indices, the sequence is labeled as *True* if the value of $T$ at the three indices is $1$, otherwise *False*. This means that the sequence label only depends on 3 elements of the whole sequence.\n",
    "\n",
    "$X := [x_1, x_2, ..., X_T]^{\\top}~~~~\\forall x_i \\in \\{0, 1 \\} \\\\$\n",
    "$f: \\{0, 1 \\}^T \\rightarrow \\{0, 1 \\}$\n",
    "\n",
    "Our goal is to find a DL architecture that could learn the underlying sparse-boolean function $f$ with least necessary number of training sample and best generalization error.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this tutorial we have already defined a s-sparse AND dataset generator class `s_Sparse_Boolean`, that can generate $m$ sequence with context length $T$ and sparsity of $s$. Here we visualize few samples and their corresponding labels. The red rectangles show the relevant indicies for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length = 30  # T: context length\n",
    "s_sparse = 3  # s: sparsity (number of function-relevant indices)\n",
    "n_sequences = 10  # m: number of samples (sequences)\n",
    "data_gen = s_Sparse_Boolean(context_length, s_sparse)\n",
    "X_, y_ = data_gen.generate(n_sequences, verbose=False)\n",
    "correct_ids = data_gen.f_i\n",
    "print(f\"Target (function-relevant indices) indices: {correct_ids}\")\n",
    "\n",
    "plot_samples(X_, y_, correct_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Video 1: Video 1 Name  # put in the title of your video\n",
    "# note the libraries are imported here on purpose\n",
    "\n",
    "###@@@ for konstanine. a question, why isn't this above in the list of cells?\n",
    "\n",
    "from ipywidgets import widgets\n",
    "from IPython.display import YouTubeVideo\n",
    "from IPython.display import IFrame\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "class PlayVideo(IFrame):\n",
    "  def __init__(self, id, source, page=1, width=400, height=300, **kwargs):\n",
    "    self.id = id\n",
    "    if source == 'Bilibili':\n",
    "      src = f'https://player.bilibili.com/player.html?bvid={id}&page={page}'\n",
    "    elif source == 'Osf':\n",
    "      src = f'https://mfr.ca-1.osf.io/render?url=https://osf.io/download/{id}/?direct%26mode=render'\n",
    "    super(PlayVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "\n",
    "def display_videos(video_ids, W=400, H=300, fs=1):\n",
    "  tab_contents = []\n",
    "  for i, video_id in enumerate(video_ids):\n",
    "    out = widgets.Output()\n",
    "    with out:\n",
    "      if video_ids[i][0] == 'Youtube':\n",
    "        video = YouTubeVideo(id=video_ids[i][1], width=W,\n",
    "                             height=H, fs=fs, rel=0)\n",
    "        print(f'Video available at https://youtube.com/watch?v={video.id}')\n",
    "      else:\n",
    "        video = PlayVideo(id=video_ids[i][1], source=video_ids[i][0], width=W,\n",
    "                          height=H, fs=fs, autoplay=False)\n",
    "        if video_ids[i][0] == 'Bilibili':\n",
    "          print(f'Video available at https://www.bilibili.com/video/{video.id}')\n",
    "        elif video_ids[i][0] == 'Osf':\n",
    "          print(f'Video available at https://osf.io/{video.id}')\n",
    "      display(video)\n",
    "    tab_contents.append(out)\n",
    "  return tab_contents\n",
    "\n",
    "# curriculum or production team will provide these ids\n",
    "video_ids = [('Youtube', '<video_id_1>'), ('Bilibili', '<video_id_2>'), ('Osf', '<video_id_3>')]\n",
    "tab_contents = display_videos(video_ids, W=854, H=480)\n",
    "tabs = widgets.Tab()\n",
    "tabs.children = tab_contents\n",
    "for i in range(len(tab_contents)):\n",
    "  tabs.set_title(i, video_ids[i][0])\n",
    "display(tabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_Video_1_Name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Exercise 1.1: Multi-Layer Perceptron\n",
    "\n",
    "Multi-Layer Percepton (MLP) layers are a key part of most deep learning architectures. Theoretically, a neural network with one wide enough (i.e. with enough learnable parameters) hidden MLP layer, enough training samples, and training iterations, is a universal function approximator. But in practice, these models tend to be extremely limited in their representation strength. Nonetheless, they are often a good place to start from.\n",
    "\n",
    "Below we will train an MLP with one hidden layer on the s-sparse AND dataset. You should be able to evaluate the performance and generalization of the model for the given task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion\n",
    "* What are the symmetries in a fully connected MLP?\n",
    "* Why do MLPs often fall behind other architectures in representation learning and generalization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task\n",
    "First, evaluate and discuss the performance of the model! Then Change the setup (through hyperparameters) such that the model performs better on the validation set (empirical generalization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Hyperparameters\n",
    "context_length = 30  # T: context length\n",
    "s_sparse = 3  # s: sparsity (number of function-relevant indices)\n",
    "B_train = 50  # batch size for training (number of training samples)\n",
    "B_valid = 500  # batch size for validation  (number of validation samples)\n",
    "hidden_layers = [512, 128, 64]  # the number of hidden units in each layer [H1, H2, ...]\n",
    "etta = 1e-3  # learning rate\n",
    "n_epochs = 500  # number of epochs\n",
    "\n",
    "# # Data generation\n",
    "data_gen = s_Sparse_Boolean(context_length, s_sparse)\n",
    "X_train, y_train = data_gen.generate(B_train, verbose=False)\n",
    "X_valid, y_valid = data_gen.generate(B_valid, verbose=False)\n",
    "\n",
    "# # model, optimizer, and criterion \n",
    "mlp_model = BinaryMLP(context_length, hidden_layers, 1)\n",
    "optimizer = torch.optim.Adam(mlp_model.parameters(), lr=etta, weight_decay=1e-4)\n",
    "criterion = torch.nn.BCELoss()\n",
    "print(f\"Number of model's learnable parameters: {get_n_parameters(mlp_model)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Training and evaluation\n",
    "t_loss, t_acc = trainer(mlp_model, optimizer, criterion, n_epochs, X_train.float(), y_train, device=device, verbose=False)\n",
    "v_loss, v_acc = evaluator(mlp_model, criterion, X_valid.float(), y_valid, device=device)\n",
    "print(f\"Training loss: {t_loss[-1]:.3f}, accuracy: {t_acc[-1]:.3f}\")\n",
    "print(f\"Validation loss: {v_loss:.3f}, accuracy: {v_acc:.3f}\")\n",
    "plot_loss_accuracy(t_loss, t_acc, v_loss, v_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1.2: Self-attention Transformers\n",
    "\n",
    "Self-attention is the heart of all transformer models, including ViT, ChatGPT, and Dino. For this tutorial, we will limit ourselves to a very simple implementation of the self-attention transformer for classification. Although the model below  has only one self-attention block, it can be easily extended to more state-of-the-art architectures. We also use positional embedding (learnable embedding), as opposed to positional encoding.\n",
    "\n",
    "> TODO the attention figure!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task\n",
    "Look through the model and the figure, and make sure that you know the purpose of each component, try to follow the information flow and appreciate the simplicity of underlying components that give rise to such a powerful architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinarySAT(torch.nn.Module):\n",
    "    \"\"\"Binary Self-Attention Transformer\n",
    "    \"\"\"\n",
    "    def __init__(self, T: int, d: int, n_heads: int, n: int):\n",
    "        super().__init__()\n",
    "        self.T = T  # context length\n",
    "        self.E = T + 1  # effective length (including cls token)\n",
    "        self.d = d  # embedding size\n",
    "        self.n_heads = n_heads  # number of heads\n",
    "        self.scale = (d // n_heads) ** -0.5  # scaling factor (1 / sqrt(d_k))\n",
    "        self.n = n  # number of hidden units\n",
    "        assert d % n_heads == 0, \"embedding size must be divisible by number of heads\"\n",
    "        self.v = 2  # vocabulary size (binary input, 0 or 1)\n",
    "        att_drop=0.1\n",
    "        out_drop=0.1\n",
    "        mlp_drop=0.1\n",
    "        ln_eps=1e-6\n",
    "\n",
    "        # embedding layers\n",
    "        self.toke = torch.nn.Embedding(2, d)  # token embedding\n",
    "        self.cls = torch.nn.Parameter(torch.randn(1, 1, d))  # \"cls / class / global\" learnable token\n",
    "        self.pose = torch.nn.Parameter(torch.randn(1, T + 1, d))  # positional embedding\n",
    "        self.norm1 = torch.nn.LayerNorm(d, eps=ln_eps)  # [https://arxiv.org/pdf/2002.04745.pdf]\n",
    "\n",
    "        # self-attention layers\n",
    "        self.qkv = torch.nn.Linear(d, 3 * d)  # query, key, value layers\n",
    "        self.dropout_attn = torch.nn.Dropout(att_drop)\n",
    "        self.proj = torch.nn.Linear(d, d)  # projection layer\n",
    "        self.dropout_out = torch.nn.Dropout(out_drop)\n",
    "        self.norm2 = torch.nn.LayerNorm(d, eps=ln_eps)\n",
    "\n",
    "        # MLP layers\n",
    "        self.mlp_l1 = torch.nn.Linear(d, n)\n",
    "        self.mlp_l2 = torch.nn.Linear(n, d)\n",
    "        self.dropout_mlp = torch.nn.Dropout(mlp_drop)\n",
    "        self.norm3 = torch.nn.LayerNorm(d, eps=1e-6)\n",
    "\n",
    "        # clasification layer\n",
    "        self.head = torch.nn.Linear(d, 1)\n",
    "\n",
    "        # initialize weights and biases (per description in the paper)\n",
    "        torch.nn.init.normal_(self.toke.weight, std=0.02)\n",
    "        torch.nn.init.normal_(self.pose, std=0.02)\n",
    "        torch.nn.init.normal_(self.cls, std=0.02)\n",
    "        torch.nn.init.ones_(self.norm1.weight)\n",
    "        torch.nn.init.zeros_(self.norm1.bias)\n",
    "\n",
    "        torch.nn.init.normal_(self.qkv.weight, std=0.02)\n",
    "        torch.nn.init.zeros_(self.qkv.bias)\n",
    "        torch.nn.init.normal_(self.proj.weight, std=0.02)\n",
    "        torch.nn.init.zeros_(self.proj.bias)\n",
    "        torch.nn.init.ones_(self.norm2.weight)\n",
    "        torch.nn.init.zeros_(self.norm2.bias)\n",
    "\n",
    "        torch.nn.init.normal_(self.mlp_l1.weight, std=0.02)\n",
    "        torch.nn.init.zeros_(self.mlp_l1.bias)\n",
    "        torch.nn.init.normal_(self.mlp_l2.weight, std=0.02)\n",
    "        torch.nn.init.zeros_(self.mlp_l2.bias)\n",
    "        torch.nn.init.ones_(self.norm3.weight)\n",
    "        torch.nn.init.zeros_(self.norm3.bias)\n",
    "\n",
    "        torch.nn.init.normal_(self.head.weight, std=0.02)\n",
    "        torch.nn.init.zeros_(self.head.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Embedding\n",
    "        B = x.size(0)  # batch size\n",
    "        x = self.toke(x)\n",
    "        x = torch.cat([x, self.cls.expand(B, -1, -1)], dim=1)\n",
    "        x = x + self.pose\n",
    "    \n",
    "        # Transformer Block\n",
    "        # # (Scaled Dot-Product Attention)\n",
    "        norm_x = self.norm1(x)  # [https://arxiv.org/pdf/2002.04745.pdf]\n",
    "        q, k, v = self.qkv(norm_x).view(B, self.E, 3, self.n_heads, -1).unbind(dim=2)\n",
    "        logits = torch.einsum(\"bthc,bshc->bhts\", q, k)  # query key product\n",
    "        logits *= self.scale  # normalize against staturation\n",
    "        attn = torch.softmax(logits, dim=-1)\n",
    "        attn = self.dropout_attn(attn)\n",
    "        output = torch.einsum(\"bhts,bshc->bthc\", attn, v)  # weighted attention\n",
    "        # # concat and linear projection with residual connection\n",
    "        output = output.reshape(B, self.E, self.d)  # recombine\n",
    "        output = self.proj(output)  # linear layer projection\n",
    "        output = self.dropout_out(output)\n",
    "        x = self.norm2(x + output)  # normalization and residual connection\n",
    "\n",
    "        # MLP with residual connection\n",
    "        output = torch.relu(self.mlp_l1(x))  # nonlinear layer\n",
    "        output = self.dropout_mlp(output)\n",
    "        output = self.mlp_l2(output)  # linear layer\n",
    "        x = self.norm3(x + output)  # normalization and residual connection\n",
    "\n",
    "        # projection\n",
    "        x = self.head(x[:, -1])\n",
    "        x = torch.sigmoid(x)  # binary classification task\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can now train a self-attention model with 1-head and 1-block on the s-sparse AND dataset. The choice of hyper-parameters here is solely for the purpose of demonstration. The paper uses 16-heads and double the embedding and hidden dimensionality.\n",
    "\n",
    "#### Task:\n",
    "First, evaluate and discuss the performance of the attention model and compare the results and hyper-parameters with the MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Hyperparameters\n",
    "context_length = 30  # T: context length\n",
    "s_sparse = 3  # s: sparsity (number of function-relevant indices)\n",
    "B_train = 50  # batch size for training (number of training samples)\n",
    "B_valid = 500  # batch size for validation  (number of validation samples)\n",
    "\n",
    "embed_dim = 32  # embedding dimension\n",
    "n_heads = 1  # number of heads\n",
    "hidden_dim = 64  # number of hidden units\n",
    "\n",
    "etta = 1e-3  # learning rate\n",
    "n_epochs = 500  # number of epochs\n",
    "\n",
    "# # Data generation\n",
    "data_gen = s_Sparse_Boolean(context_length, s_sparse)\n",
    "X_train, y_train = data_gen.generate(B_train, verbose=False)\n",
    "X_valid, y_valid = data_gen.generate(B_valid, verbose=False)\n",
    "\n",
    "# # model, optimizer, and criterion \n",
    "sat_model = BinarySAT(context_length, embed_dim, n_heads, hidden_dim)\n",
    "optimizer = torch.optim.Adam(sat_model.parameters(), lr=etta, weight_decay=1e-4)\n",
    "criterion = torch.nn.BCELoss()\n",
    "print(f\"Number of model's learnable parameters: {get_n_parameters(sat_model)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Training and evaluation\n",
    "t_loss, t_acc = trainer(sat_model, optimizer, criterion, n_epochs, X_train, y_train, device=device, verbose=False)\n",
    "v_loss, v_acc = evaluator(sat_model, criterion, X_valid, y_valid, device=device)\n",
    "print(f\"Training loss: {t_loss[-1]:.3f}, accuracy: {t_acc[-1]:.3f}\")\n",
    "print(f\"Validation loss: {v_loss:.3f}, accuracy: {v_acc:.3f}\")\n",
    "plot_loss_accuracy(t_loss, t_acc, v_loss, v_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding Exercise 1: Weighted attention\n",
    "\n",
    "A common figure in attention literature is the \"Attention visualization\" which shows how the model is attending to different parts of the sequence. Here, we will call this the weighted attention defined as follow:\n",
    "\n",
    "$$W_{QK} = \\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}})$$\n",
    "\n",
    "The implementation above already includes this, but we would like to have a function that takes the input sequence and outputs the $W_{QK}$. Note that since the $W_{QK}$ is to \"re-weight\" the input sequence, it should be of shape `[B, E, E]` where `B` is the batch_size, `E` is the extended context length ($E = T + 1$ since the `cls` token is appended to the sequence!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_attention(self, x):\n",
    "    \"\"\"This function computes the weighted attention as a method for the BinarySAT class\n",
    "\n",
    "    Args:\n",
    "        x (Tensor): An array of shape (B:batch_size, T:context length) containing the input data\n",
    "\n",
    "    Returns:\n",
    "        Tensor: weighted attention\n",
    "    \"\"\"\n",
    "    assert self.n_heads == 1, \"This function is only implemented for a single head!\"\n",
    "    # Embedding\n",
    "    B = x.size(0)  # batch size\n",
    "    x = self.toke(x)  # token embedding\n",
    "    x = torch.cat([x, self.cls.expand(B, -1, -1)], dim=1)  # concatenate cls token\n",
    "    x = x + self.pose  # positional embedding\n",
    "    norm_x = self.norm1(x)  # normalization\n",
    "\n",
    "    # Scaled Dot-Product Attention (partially implemented)\n",
    "    q, k, v = self.qkv(norm_x).view(B, self.E, 3, self.d).unbind(dim=2)\n",
    "    #################################################\n",
    "    ## TODO Implement the weighted attention \n",
    "    # Fill remove the following line of code one you have completed the exercise:\n",
    "    raise NotImplementedError(\"Student exercise: say what they should have done\")\n",
    "    #################################################\n",
    "    W_qk = ...\n",
    "    return W_qk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_remove solution\n",
    "def weighted_attention(self, x):\n",
    "    \"\"\"This function computes the weighted attention as a method for the BinarySAT class\n",
    "\n",
    "    Args:\n",
    "        x (Tensor): An array of shape (B:batch_size, T:context length) containing the input data\n",
    "\n",
    "    Returns:\n",
    "        Tensor: weighted attention\n",
    "    \"\"\"\n",
    "    assert self.n_heads == 1, \"This function is only implemented for a single head!\"\n",
    "    # Embedding\n",
    "    B = x.size(0)  # batch size\n",
    "    x = self.toke(x)  # token embedding\n",
    "    x = torch.cat([x, self.cls.expand(B, -1, -1)], dim=1)  # concatenate cls token\n",
    "    x = x + self.pose  # positional embedding\n",
    "    norm_x = self.norm1(x)  # normalization\n",
    "    \n",
    "    # Scaled Dot-Product Attention (partially implemented)\n",
    "    q, k, v = self.qkv(norm_x).view(B, self.E, 3, self.d).unbind(dim=2)\n",
    "    W_qk = q @ k.transpose(-2, -1)\n",
    "    W_qk = W_qk * self.scale\n",
    "    W_qk = torch.softmax(W_qk, dim=-1)\n",
    "    return W_qk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length = 30  # T: context length\n",
    "s_sparse = 3  # s: sparsity (number of function-relevant indices)\n",
    "n_sequences = 100  # m: number of samples (sequences)\n",
    "X_, y_ = data_gen.generate(n_sequences, verbose=False)\n",
    "correct_ids = data_gen.f_i\n",
    "print(f\"Target (function-relevant indices) indices: {correct_ids}\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    w_att = weighted_attention(sat_model, X_.to(device)).cpu().detach()\n",
    "\n",
    "plot_attention_weights(w_att, correct_ids, context_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_name_of_Exercise\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1.3: Sparse Variable Creation\n",
    "\n",
    "So far, we saw that a simple transformer model can effectively learn to represent an s-sparse boolean function. Next, we would like to demonstrate the sample efficiency of self-attention for such class of functions. The paper regorously shows that the number of training samples $m$ needed to achieve a good generalization on an s-sparse function, grows only logarithmically with respect to the context length $T$. They also put forward their empirical results for the s-sparse AND dataset. Given the time and computation limits of our tutorial, we will only show their results here, but you can find the implementation and detailed results on GitHub. The Figure below is from the paper [Inductive Biases and Variable Creation in Self-Attention Mechanisms](https://proceedings.mlr.press/v162/edelman22a.html)\n",
    "\n",
    "<div>\n",
    "<img src=\"./static/Fig_2_ref2.png\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_Name_of_Discussion_Topic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Summary\n",
    "\n",
    "*Estimated timing of tutorial: [minutes]* [provide the estimated time for the completing of the entire tutorail]\n",
    "\n",
    "Have a summary of what they learned with specific points.\n",
    "\n",
    "1. Specific point A\n",
    "\n",
    "2. Specific point B"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "W1D2_Tutorial1",
   "provenance": [],
   "toc_visible": true
  },
  "kernel": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
