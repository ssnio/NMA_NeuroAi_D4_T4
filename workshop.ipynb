{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy\n",
    "import random\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneDAnd:  # 1-Dimensional AND\n",
    "    def __init__(self, T: int, s: int):\n",
    "        self.T = T # context length\n",
    "        self.s = s # sparsity\n",
    "        self.p = 0.5**(1.0/3.0)  # probability chosen for balanced data\n",
    "        self.f_i = None\n",
    "\n",
    "    def pick_an_f(self):\n",
    "        self.f_i = sorted(random.sample(range(self.T), 3))\n",
    "        self.others = list(i for i in range(self.T) if i not in self.f_i)\n",
    "\n",
    "    def generate(self, m: int, split: str = \"train\", verbose: bool = False):\n",
    "        if self.f_i is None:\n",
    "            self.pick_an_f()\n",
    "        max_try = 100\n",
    "        i_try = 0\n",
    "        while i_try < max_try:\n",
    "            i_try += 1\n",
    "            X, y = torch.zeros(m, self.T), torch.zeros(m, 1)\n",
    "            X[torch.rand(m, self.T) < self.p] = 1\n",
    "            y[X[:, self.f_i].sum(dim=1) == self.s] = 1\n",
    "            if y.sum()/m < 0.4 or y.sum()/m > 0.6:\n",
    "                verbose and print(f\"Large imbalance in the training set {y.sum()/m}, retrying...\")\n",
    "                continue\n",
    "            else:\n",
    "                verbose and print(f\"Data-label balance: {y.sum()/m}\")\n",
    "            if split == \"train\":  # currently we choose not to do this\n",
    "                bad_batch = False\n",
    "                for i in self.f_i:\n",
    "                    for o in self.others:\n",
    "                        if (X[:, i] == X[:, o]).all():\n",
    "                            verbose and print(f\"Found at least another compatible hypothesis {i} and {o}\")\n",
    "                            bad_batch = True\n",
    "                            break\n",
    "                if bad_batch:\n",
    "                    continue\n",
    "                else:\n",
    "                    break\n",
    "            else:\n",
    "                break\n",
    "        else:\n",
    "            print(\"Could not find a compatible hypothesis\")\n",
    "        return X.long(), y.float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryBERT(torch.nn.Module):\n",
    "    def __init__(self, T: int, d: int, n_heads: int, n: int):\n",
    "        super().__init__()\n",
    "        self.T = T  # context length\n",
    "        self.E = T + 1  # effective length (including cls token)\n",
    "        self.d = d  # embedding size\n",
    "        self.n_heads = n_heads  # number of heads\n",
    "        self.scale = (d // n_heads) ** -0.5  # scaling factor\n",
    "        self.n = n  # number of hidden units\n",
    "        assert d % n_heads == 0, \"embedding size must be divisible by number of heads\"\n",
    "        self.v = 2  # vocabulary size\n",
    "        att_drop=0.1\n",
    "        out_drop=0.1\n",
    "        mlp_drop=0.1\n",
    "        ln_eps=1e-6\n",
    "\n",
    "        self.toke = torch.nn.Embedding(2, d)  # token embedding\n",
    "        self.cls = torch.nn.Parameter(torch.randn(1, 1, d))  # \"cls / class / global\" learnable token\n",
    "        self.pose = torch.nn.Parameter(torch.randn(1, T + 1, d))  # positional embedding\n",
    "        self.norm1 = torch.nn.LayerNorm(d, eps=ln_eps)\n",
    "\n",
    "        self.qkv = torch.nn.Linear(d, 3 * d)  # query, key, value layers\n",
    "        self.dropout_attn = torch.nn.Dropout(att_drop)\n",
    "        self.proj = torch.nn.Linear(d, d)  # projection layer\n",
    "        self.dropout_out = torch.nn.Dropout(out_drop)\n",
    "        self.norm2 = torch.nn.LayerNorm(d, eps=ln_eps)\n",
    "\n",
    "        self.mlp_l1 = torch.nn.Linear(d, n)\n",
    "        self.mlp_l2 = torch.nn.Linear(n, d)\n",
    "        self.mlp_d = torch.nn.Dropout(mlp_drop)\n",
    "        self.norm3 = torch.nn.LayerNorm(d, eps=1e-6)\n",
    "\n",
    "        self.head = torch.nn.Linear(d, 1)\n",
    "\n",
    "        torch.nn.init.normal_(self.toke.weight, std=0.02)\n",
    "        torch.nn.init.normal_(self.pose, std=0.02)\n",
    "        torch.nn.init.normal_(self.cls, std=0.02)\n",
    "        torch.nn.init.ones_(self.norm1.weight)\n",
    "        torch.nn.init.zeros_(self.norm1.bias)\n",
    "\n",
    "        torch.nn.init.normal_(self.qkv.weight, std=0.02)\n",
    "        torch.nn.init.zeros_(self.qkv.bias)\n",
    "        torch.nn.init.normal_(self.proj.weight, std=0.02)\n",
    "        torch.nn.init.zeros_(self.proj.bias)\n",
    "        torch.nn.init.ones_(self.norm2.weight)\n",
    "        torch.nn.init.zeros_(self.norm2.bias)\n",
    "\n",
    "        torch.nn.init.normal_(self.mlp_l1.weight, std=0.02)\n",
    "        torch.nn.init.zeros_(self.mlp_l1.bias)\n",
    "        torch.nn.init.normal_(self.mlp_l2.weight, std=0.02)\n",
    "        torch.nn.init.zeros_(self.mlp_l2.bias)\n",
    "        torch.nn.init.ones_(self.norm3.weight)\n",
    "        torch.nn.init.zeros_(self.norm3.bias)\n",
    "\n",
    "        torch.nn.init.normal_(self.head.weight, std=0.02)\n",
    "        torch.nn.init.zeros_(self.head.bias)\n",
    "\n",
    "    def embed(self, x):\n",
    "        B = x.size(0)  # batch size\n",
    "        x = self.toke(x)\n",
    "        x = torch.cat([x, self.cls.expand(B, -1, -1)], dim=1)\n",
    "        x = x + self.pose\n",
    "        return x\n",
    "    \n",
    "    def attend(self, x):\n",
    "        B = x.size(0)  # batch size\n",
    "        norm_x = self.norm1(x)  # [https://arxiv.org/pdf/2002.04745.pdf]\n",
    "        q, k, v = self.qkv(norm_x).view(B, self.E, 3, self.n_heads, -1).unbind(dim=2)\n",
    "        # return q, k\n",
    "        logits = torch.einsum(\"bthc,bshc->bhts\", q, k)\n",
    "        logits *= self.scale  # normalize against staturation\n",
    "        attn = torch.softmax(logits, dim=-1)\n",
    "        return attn\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Embedding\n",
    "        B = x.size(0)  # batch size\n",
    "        x = self.toke(x)\n",
    "        x = torch.cat([x, self.cls.expand(B, -1, -1)], dim=1)\n",
    "        x = x + self.pose\n",
    "    \n",
    "        # Transformer (Scaled Dot-Product Attention) with residual connection\n",
    "        norm_x = self.norm1(x)  # [https://arxiv.org/pdf/2002.04745.pdf]\n",
    "        q, k, v = self.qkv(norm_x).view(B, self.E, 3, self.n_heads, -1).unbind(dim=2)\n",
    "        logits = torch.einsum(\"bthc,bshc->bhts\", q, k)\n",
    "        logits *= self.scale  # normalize against staturation\n",
    "        attn = torch.softmax(logits, dim=-1)\n",
    "        attn = self.dropout_attn(attn)\n",
    "        output = torch.einsum(\"bhts,bshc->bthc\", attn, v)  # target source\n",
    "        output = output.reshape(B, self.E, self.d)  # recombine\n",
    "        output = self.proj(output)\n",
    "        output = self.dropout_out(output)\n",
    "        x = self.norm2(x + output)\n",
    "\n",
    "        # MLP with residual connection\n",
    "        output = self.mlp_l2(self.mlp_d(torch.relu(self.mlp_l1(x))))\n",
    "        x = self.norm3(x + output)\n",
    "\n",
    "        # projection\n",
    "        x = self.head(x[:, -1])\n",
    "        x = torch.sigmoid(x)  # binary classification task\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_acc(y_hat, y):\n",
    "    y_ = y_hat.round()\n",
    "    TP_TN = (y_ == y).float().sum().item()\n",
    "    FP_FN = (y_ != y).float().sum().item()\n",
    "    assert TP_TN + FP_FN == y.numel(), f\"{TP_TN + FP_FN} != {y.numel()}\"\n",
    "    return TP_TN / y.numel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf = torch.Tensor([0.1, 0.2, 0.8, 0.9])\n",
    "sfy = torch.Tensor([0, 0, 0, 0])\n",
    "print(bin_acc(sf, sfy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_parameters(model: torch.nn.Module):\n",
    "    i = 0\n",
    "    for par in model.parameters():\n",
    "        i += par.numel()\n",
    "    return i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model):\n",
    "    torch.save(model.state_dict(), 'model_states.pt')\n",
    "\n",
    "def load_model(model):\n",
    "    model_states = torch.load('model_states.pt')\n",
    "    model.load_state_dict(model_states)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluator(model, criterion, X_v, y_v, device=\"cpu\"):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    X_v, y_v = X_v.to(device), y_v.to(device)\n",
    "    with torch.no_grad():\n",
    "        y_hat = model(X_v)\n",
    "        loss = criterion(y_hat.squeeze(), y_v.squeeze())\n",
    "        acc = bin_acc(y_hat, y_v)\n",
    "    return loss.item(), acc\n",
    "\n",
    "\n",
    "def trainer(model, optimizer, criterion, n_epochs, X_t, y_t, X_v, y_v, device=\"cpu\", verbose=False):\n",
    "    # # book keeping\n",
    "    train_loss, valid_loss = [], []\n",
    "    train_acc, valid_acc = [], []\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    X_t, y_t = X_t.to(device), y_t.to(device)\n",
    "    X_v, y_v = X_v.to(device), y_v.to(device)\n",
    "    for i in range(n_epochs):\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        y_hat = model(X_t)\n",
    "        loss_t = criterion(y_hat.squeeze(), y_t.squeeze())\n",
    "        loss_t.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        if (i + 1) % 10 == 0 or i == 0:\n",
    "            train_loss.append(loss_t.item())\n",
    "            train_acc.append(bin_acc(y_hat, y_t))\n",
    "            model.eval()\n",
    "            loss_v, acc_v = evaluator(model, criterion, X_v, y_v, device)\n",
    "            valid_loss.append(loss_v)\n",
    "            valid_acc.append(acc_v)\n",
    "            verbose and print(f\"Epoch {i + 1:04d}:\"\n",
    "            f\" Train loss: {train_loss[-1]:.6f} acc: {train_acc[-1]:.3f}\"\n",
    "            f\" Valid loss: {valid_loss[-1]:.6f} acc: {valid_acc[-1]:.3f}\")\n",
    "            model.train()\n",
    "    model.eval()\n",
    "    return train_loss, valid_loss, train_acc, valid_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")  # NVIDIA GPU\n",
    "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "    device = torch.device(\"mps\")  # Apple Silicon (Metal)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 30  # context length\n",
    "s = 3  # sparsity\n",
    "B_t = 50  # batch size for training\n",
    "B_v = 1000  # batch size for validation\n",
    "data_gen = OneDAnd(T, s)\n",
    "X_t, y_t = data_gen.generate(B_t, split=\"train\", verbose=True)\n",
    "X_v, y_v = data_gen.generate(B_v, split=\"valid\", verbose=True)\n",
    "correct_ids = data_gen.f_i\n",
    "print(f\"Target indices: {correct_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 64  # embedding size\n",
    "n_heads = 16  # number of heads\n",
    "n = 64  # number of hidden units\n",
    "model = BinaryBERT(T, d, n_heads, n)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, betas=(0.9, 0.999), weight_decay=1e-4)\n",
    "criterion = torch.nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of parameters: {get_n_parameters(model)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 1000\n",
    "t_loss, v_loss, t_acc, v_acc = trainer(model, optimizer, criterion, n_epochs, X_t, y_t, X_v, y_v, device=device, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 4))\n",
    "plt.suptitle(\"Training and Validation for the Transformer Model\")\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(t_loss, label=\"Training loss\", color=\"red\")\n",
    "plt.plot(v_loss, label=\"Valididation loss\", color=\"blue\")\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xticks([])\n",
    "plt.legend()\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(t_acc, label=\"Training accuracy\", color=\"red\", linestyle=\"dotted\")\n",
    "plt.plot(v_acc, label=\"Validation accuracy\", color=\"blue\", linestyle=\"--\")\n",
    "plt.xticks([])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_ids = data_gen.f_i\n",
    "print(f\"Target indices: {correct_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_plot = 10\n",
    "X_plot, y_plot = data_gen.generate(B_plot, split=\"valid\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(16,2), sharey=True)\n",
    "rects = []\n",
    "for ri in correct_ids:\n",
    "    rects.append(plt.Rectangle((ri-0.5, -0.5), 1, X_plot.size(0), edgecolor=\"red\", alpha=1.0, fill=False, linewidth=2))\n",
    "axs[0].imshow(X_plot, cmap=\"binary\")\n",
    "for rect in rects:\n",
    "    axs[0].add_patch(rect)\n",
    "# axs[0].axis(\"off\")\n",
    "axs[0].set_yticks([])\n",
    "axs[0].set_xticks([])\n",
    "axs[0].set_ylabel(\"Context\")\n",
    "axs[0].set_xlabel(\"Samples\")\n",
    "axs[1].imshow(y_plot, cmap=\"binary\")\n",
    "axs[1].add_patch(plt.Rectangle((-0.5, -0.5), 1, X_plot.size(0), edgecolor=\"black\", alpha=1.0, fill=False, linewidth=2))\n",
    "axs[1].yaxis.set_label_position(\"right\")\n",
    "axs[1].set_ylabel(\"Labels\")\n",
    "axs[1].set_yticks([])\n",
    "axs[1].set_xticks([])\n",
    "plt.subplots_adjust(wspace=-1)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    o_emb = model.embed(X_plot.to(device)).cpu().detach()\n",
    "    o_att = model.attend(o_emb.to(device)).cpu().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o_att.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o_att_flatten = o_att.view(-1, T+1)\n",
    "n_weights = o_att_flatten.size(0)\n",
    "fig, ax = plt.subplots(figsize=(9, 5))\n",
    "for i in range(T+1):\n",
    "    ax.scatter(torch.full((n_weights, ), i) , o_att_flatten[:, i], alpha=0.1, c='blue')\n",
    "rects = []\n",
    "for ri in correct_ids:\n",
    "    rects.append(plt.Rectangle((ri-0.5, 1e-6), 1.0, 2.0, edgecolor=\"red\", alpha=1.0, fill=False, linewidth=2))\n",
    "for rect in rects:\n",
    "    ax.add_patch(rect)\n",
    "plt.yscale(\"log\")\n",
    "plt.ylim(1e-6, 2)\n",
    "plt.title(\"Attention weights\")\n",
    "plt.xlabel(\"Boolean input index t\")\n",
    "plt.ylabel(\"Attention weight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryMLP(torch.nn.Module):\n",
    "    def __init__(self, in_dims, h_dims, out_dims, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.in_dims = in_dims\n",
    "        self.h_dims = h_dims\n",
    "        self.out_dims = out_dims\n",
    "\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        self.layers.append(torch.nn.Linear(in_dims, h_dims[0]))\n",
    "        torch.nn.init.normal_(self.layers[-1].weight, std=0.02)\n",
    "        torch.nn.init.zeros_(self.layers[-1].bias)\n",
    "        self.layers.append(torch.nn.GELU())\n",
    "        self.layers.append(torch.nn.Dropout(dropout))\n",
    "        for i in range(len(h_dims) - 1):\n",
    "            self.layers.append(torch.nn.Linear(h_dims[i], h_dims[i+1]))\n",
    "            torch.nn.init.normal_(self.layers[-1].weight, std=0.02)\n",
    "            torch.nn.init.zeros_(self.layers[-1].bias)\n",
    "            self.layers.append(torch.nn.GELU())\n",
    "            self.layers.append(torch.nn.Dropout(dropout))\n",
    "        self.layers.append(torch.nn.Linear(h_dims[-1], out_dims))\n",
    "        self.layers.append(torch.nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 30  # context length\n",
    "s = 3  # sparsity\n",
    "B_t = 50  # batch size for training\n",
    "B_v = 500  # batch size for validation\n",
    "data_gen = OneDAnd(T, s)\n",
    "X_t, y_t = data_gen.generate(B_t, split=\"train\", verbose=True)\n",
    "X_v, y_v = data_gen.generate(B_v, split=\"valid\", verbose=True)\n",
    "correct_ids = data_gen.f_i\n",
    "print(f\"Target indices: {correct_ids}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model = BinaryMLP(T, [128, 128], 1)\n",
    "optimizer = torch.optim.Adam(mlp_model.parameters(), lr=1e-3, betas=(0.9, 0.999), weight_decay=1e-4)\n",
    "criterion = torch.nn.BCELoss()\n",
    "print(f\"Number of parameters: {get_n_parameters(mlp_model)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 1000\n",
    "t_loss, v_loss, t_acc, v_acc = trainer(mlp_model, optimizer, criterion, n_epochs, X_t.float(), y_t, X_v.float(), y_v, device=device, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 4))\n",
    "plt.suptitle(\"Training and Validation for the Transformer Model\")\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(t_loss, label=\"Training loss\", color=\"red\")\n",
    "plt.plot(v_loss, label=\"Valididation loss\", color=\"blue\")\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xticks([])\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(t_acc, label=\"Training accuracy\", color=\"red\", linestyle=\"dotted\")\n",
    "plt.plot(v_acc, label=\"Validation accuracy\", color=\"blue\", linestyle=\"--\")\n",
    "plt.xticks([])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = data_gen.generate(B_v, split=\"test\", verbose=True)\n",
    "evaluator(model, criterion, X_test, y_test, device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
